{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6073433824299648323\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7771504640\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3697940324540693603\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess= tf.Session(config=config)\n",
    "\n",
    "import sys\n",
    "from tensorflow.python.client import device_lib\n",
    "print (device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일로부터 텍스트 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before deleting : 4368\n",
      "after deleting : 4363\n",
      "100년 전 3월 1일 당시와 다른 것은 국내 정세 뿐만이 아니었겠죠.오늘도 뿌연 하늘 속에 미세먼지가 심상치 않았습니다.특히 내륙 일부 지역은 관측 사상 최고치를 경신하며, 이례적으로 중국 수도권보다 높은 미세먼지 농도를 보였는데요.원인이 무엇인지 이정훈 기상전문기자가 전해드립니다.봄의 시작부터 전국이 고농도 미세먼지로 뒤덮였습니다.여의도 빌딩 숲도 짙은 미세먼지에 잠겼습니다.나들이 나온 시민들도 봄기운을 느끼기 전에 미세먼지 걱정부터 앞섭니다.오늘 수도권과 충청, 호남 지방의 초미세먼지 농도는 평소 3배가 넘는 매우 나쁨 수준까지 높아졌습니다.특히 세종과 대전, 광주는 이 지역 초미세먼지 관측을 시작한 2015년 이후 가장 높은 농도를 기록했습니다.특히 이번에는 이례적으로 내륙 지역의 농도가 서해안과 수도권은 물론 중국 베이징이나 산둥성보다도 높은 수준을 보였습니다.바람이 멎은 가운데 동쪽이 산맥에 막힌 지형 효과가 더해졌기 때문으로 분석됩니다.대기를 정체하게 하는 고기압이 서해 상에 머물며 중국발 오염 물질을 몰고온 데다 국내 오염 물질까지 내륙 지역에 가둔 겁니다.내일도 서쪽 지역에 고농도 미세먼지가 머물 것으로 예상되면서 수도권과 충청 지역은 비상저감조치가 이어집니다.국립환경과학원은 일요일인 모레쯤 미세먼지가 해소될 것으로 내다봤습니다.KBS 뉴스 이정훈입니다.이정훈 기자  \n",
      "\n",
      "두렵다, 3월 가장 심하다는데벌써부터 숨쉴때마다 가슴팍이 아프고 눈, 코 모두 따갑다, 진짜 어렸을 땐 상상도 못했던 일인데내 나라 전체의 땅이 먼지에 갇혀 살 줄이야,살다 살다 이젠 공기까지 힘들게하네\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "특히 100년전 3.1 운동은 민족독립의 의미를 넘어서는 현재적 가치가 있습니다.왕정, 전제군주정이었던 전근대 체제를 넘어서서 민주공화정의 기틀을 닦은 전환점이 됐다는 평가를 받고 있습니다.3.1 운동의 가장 큰 특징이 몇몇 지식인의 선언적 운동이 아니라 남녀노소 다양한 계층이 참여했다는 점인데요.모든 권력은 국민으로부터 나온다는 주권의식과 민주주의 발전의 원동력이 됐습니다.유동엽 기자가 분석했습니다.이곳 서대문 형무소는 일제강점기 독립투사들의 한이 어린 곳이죠.당시 수용 정원은 5백 명이었습니다.하지만 삼일 운동 직후에는 잡혀 온 사람들이 크게 늘어서, 수감자가 3천여 명에 달했습니다.정원의 6배가 넘게 수용된 옥사 안은 어떤 모습이었을까요?가로세로 3m가 안 되는 공간에 열 명 넘게 갇혀 누울 자리조차 없었습니다.당시 여기 갇힌 독립운동가들은 수형자 카드로 확인할 수 있는데요,사진과 함께 신분, 직업 등의 신상정보가 적혀있습니다.삼일운동에 주도적으로 참여해 옥고까지 치렀던 천여 명의 카드를 분석해봤더니 평민이, 대다수인 85 나 됐습니다.몇몇 지식인이 아닌 민초들이 앞장서 대거 참여한 겁니다.20 30대가 가장 많기는 했지만 15살 학생부터 69살 노인까지 있었습니다.10대부터 60대까지 남녀노소 구분 없이 나섰다는 증겁니다.직업은 농업이 절반 이상이긴 했지만, 상인과 직공, 마차꾼까지 80여 가지, 당시 있던 대부분 직업이 총 망라됐습니다.지역적으로도, 서울과 경기를 시작으로 북쪽의 함경도에서부터 남쪽의 전라와 경상도까지 천9백 건 넘는 만세 시위가 있었습니다.시 군 가운데 96 , 전국 방방곡곡에서 만세 함성이 울려 퍼졌습니다.참여 인원도 많게는 200여만 명으로 추산되는데 당시 인구의 10분의 1이 넘는 수준입니다.이렇게 100년 전 모두가 하나로 뭉쳐 외친 대한 독립 만세 .오늘을 살아가는 우리에겐 어떤 의미가 있을까요?장혁진 기자가 살펴봤습니다. 민중의 힘, 민주주의 발전 원동력 조선이 독립한 나라이며, 조선인이 이 나라의 주인임을 선언한다. 탑골공원에서 3.1운동의 불씨가 타올랐습니다.대한문과 미국영사관을 거치면서 거리는 태극기로 물들고, 프랑스 영사관에 다다랐습니다.학생 박승영은 영사에게 독립선언문을 전달합니다. 조선은 오늘 독립을 선언하고, 사람들은 모두 독립을 희망하고 있다. 이 뜻을 본국 프랑스에 알려 달라. 이들은 지배받는 백성이 아닌 민족 운명을 스스로 결정하는 나라를 외쳤습니다.특히, 평화를 추구한 3.1운동의 비폭력 투쟁 방식은 세계 혁명사에서 유례를 찾기 힘듭니다.3.1운동은 인간 자유에 대한 외침이었고, 계층과 성별을 뛰어넘어 모두가 참여했습니다.그 정신은 임시정부 헌장의 민주 공화제 로 발현돼 지금 헌법까지 이어집니다.특히 부조리한 현실을 민중의 힘으로 바꾸겠다는 주권의식은 민주화 운동의 굽이굽이마다 스며들어 민주주의 발전의 원동력이 됐습니다.만세를 외치며 모두가 하나가 됐던 100년 전 오늘.분열과 갈등의 시대에 사는 우리에게 미래에 나아갈 방향을 비춰줍니다.KBS 뉴스 장혁진입니다.유동엽 기자 장혁진 기자 . \n",
      "\n",
      "3.1운동은 인간 자유에 대한 외침 이었고, 계층과 성별을 뛰어넘어 모두가 참여했습니다. 그 정신은 임시정부 헌장의 민주 공화제 로 발현돼 지금 헌법까지 이어집니다.특히 부조리한 현실을 민중의 힘으로 바꾸겠다는 주권의식은 민주화 운동의 굽이굽이마다 스며들어 민주주의 발전의 원동력이 됐습니다.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "동영상 뉴스 석방할 때쯤 술 깨서 음주 측정 끝까지 거부오늘 새벽 서울 동작구에서 20대가 몰던 차량이 앞차를 들이받아 그 사고 여파로 주변 전기 공급이 4시간 동안 끊겼습니다. 술을 마신 것으로 의심되는 20대 운전자는 경찰의 음주 측정도 끝까지 거부했습니다.정다은 기자가 보도합니다.차량 사고 현장에서 미란다 원칙을 고지한 경찰이 20대 남성을 체포합니다.욕설을 내뱉으며 거세게 저항하다 뒤 수갑까지 찬 뒤에야 연행됩니다.오늘 새벽 0시 10분쯤 서울 흑석동에서 26살 김 모 씨의 벤츠 차량이 신호대기 중이던 승용차를 들이받았습니다.사고 충격으로 피해 차량이 튕겨 나가면서 인도에 설치된 전력 개폐기를 덮쳤고 근처 주택 등 54세대의 전기 공급이 4시간 동안 끊겼습니다.피해차량 운전자는 치아 등을 다쳐 병원 치료를 받고 있습니다.경찰은 운전자 김 씨가 술을 마신 채 운전하다 사고를 낸 것으로 보고 있습니다.경찰은 일단 김 씨를 귀가시킨 뒤 음주측정 거부 등의 혐의로 다시 불러 조사할 계획입니다. 정다은 기자 SBS SBS Digital News Lab. 무단복제 및 재배포 금지 \n",
      "\n",
      "동작서 맞냐는거보니까 이새끼 윗대가리 아들인갑네\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "동영상 뉴스다시 탑골공원입니다. 오늘 대통령도 3 1절 기념사에서 잘못된 과거를 정리하자고 얘기를 했는데 세월만 흘렀을 뿐, 바로잡지 못한 역사가 하나둘이 아닙니다. 당장 일제가 강제로 해외로 끌고 갔다가 숨진 우리 조상들 중에 유해로나마 돌아온 경우는 2 3 정도입니다. 우리가 요즘 여행 많이 가는 중국 하이난에 조선인 집단 매장지가 아직도 방치돼 있습니다.고정현 기자가 대표적인 사례로 이곳을 다녀왔습니다.중국 하이난성 싼야시 중심에서 차로 15분 거리, 지금도 조선촌 으로 불리는 곳에 일제시대 강제징용으로 끌려온 조선인 1천 명이 집단매장됐다는 뜻의 천인갱 이 있습니다.1995년 중국 하이난성 정부에서 엮은 일제 피해자 구술집을 통해 처음 존재가 드러났습니다.언론을 통해 국내에서도 알려지자 하이난성에서 망고농장을 운영하던 우리 기업이 묘역화를 해도 좋다는 중국 정부의 허가를 받아 담장을 치고 추모관까지 세웠습니다.하지만 지금은 담장 곳곳이 허물어졌고 현지 주민이 묘를 쓰지 못해 몰래 버린 관들이 쌓여 있습니다.사실상 버려진 추모관은 을씨년스럽기까지 합니다.영락제, 영원히 편안하고 즐거운 곳에 모시겠다 이 정도 뜻으로 해석될 수 있을 것 같은데요, 이 건물 바로 옆 부지는 여전히 수많은 조선인 강제 징용 피해자가 일제에 죽임을 당한 채 그대로 묻혀 있을 것으로 추정되는 곳입니다.하지만 보시는 것처럼 근처 주민들이 농사를 지었는지 밭이랑과 고랑의 흔적이 그대로 남아 있는 상태고요, 근처 축사에서 흘러나온 오물이 가득 쌓여 심한 악취를 계속 풍기고 있는 상태입니다.1939년 하이난섬을 정복한 일본은 태평양전쟁 막바지인 43년부터 조선 전체 수형자의 10 에 달하는 2천 명을 조선 보국대 라는 이름으로 하이난섬에 보냈습니다.이들 중에는 특히 독립운동을 하다 잡힌 사상범도 상당수 포함됐을 것으로 추정됩니다.위안부도 최소 2백10명이 끌려가 1백30명 넘게 돌아오지 못한 것으로 추산됐습니다.대부분 비행장 건설이나 탄광 작업 같은 강제 노역에 동원됐는데 사망률이 군인 전사율과 같은 60 에 달할 만큼 혹독했습니다.생존자와 현지 주민은 당시 일본군이 도망자들을 죽이는 것은 물론 살아 있는 사람도 병에 걸리면 전염을 우려해 학살했다고 증언합니다.당시 귀환 기록이 없는 강제 징용자가 1천2백 명이 넘는 만큼 천인갱 은 이들의 유해일 가능성이 매우 높습니다.20년 전 우리나라 한 기업이 확보한 천인갱 부지는 3만3천 에 달합니다.하지만 지금 천인갱이라고 불리는 부지는 20분의 1에 불과한 1천6백 로 쪼그라들었습니다.해당 기업이 자금난으로 5년간 토지사용료를 내지 못하자 중국 정부가 허가를 취소한 겁니다.그나마 남은 부지도 지키기가 쉽지 않은 상태입니다.허허벌판이었던 천인갱 근처에는 고속도로가 생겼고 본토로 들어가는 고속철도 역까지 들어왔습니다.코앞까지 들어선 초고층 아파트가 언제 비집고 들어올지 모르는 상황인 겁니다.우리 역사의 아픔과 민족의 한이 고스란히 묻혀있는 천인갱 은 무관심 속에 그대로 지워져 버릴 위기에 처했습니다. 30cm만 파도 유해 나오는데 24년간 방치된 까닭은 고정현 기자 SBS SBS Digital News Lab. 무단복제 및 재배포 금지 \n",
      "\n",
      "독립군 재조명하고 친일명부 재조사해라 \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "동영상 뉴스오늘 울산에서는 강제징용 피해자들을 기리는 동상이 세워졌습니다. 전국에서 6번째인데요. 지난해 설치했다가 바로 철거됐던 부산 일본영사관 앞에서도 노동자상을 다시 세우려 했지만, 경찰과 대치 끝에 무산됐습니다.배승주 기자입니다.곡괭이를 두 손으로 움켜쥔 청년, 몸은 깡마른 노인의 모습입니다.비좁은 틈에 몸을 구겨 넣고 땅을 파 내려갑니다.오늘 울산대공원 동문에 건립된 강제징용노동자상입니다.여전히 책임을 회피하는 일본에 대한 피해자의 분노와 당시 열악한 노동환경을 표현했습니다.2016년 일본 단바 광산을 시작으로 국내에서는 2년 전 서울 용산역과 인천, 제주, 창원, 부산에 이어 6번째입니다.부산 일본영사관 앞 소녀상 옆에서도 민주노총과 시민단체 회원들이 나서 노동자상 재설치를 시도했습니다.지난해 5월 1일 노동절 때 이미 자리를 잡은 소녀상 옆에 설치했지만 부산 동구청에 의해 철거된 노동자상입니다.하지만 소녀상 쪽으로 가려는 행진 대열이 경찰에 막혀 결국 100여m 떨어진 공원에 임시 설치했습니다.이 과정에서 집회 참가자와 경찰이 1시간 정도 대치하기도 했습니다.한편, NHK와 아사히신문 등 일본 언론에서도 현장에 나와 높은 관심을 보였습니다.배승주 Copyright by JTBC and JTBC Content Hub Co, Ltd. All Rights Reserved. 무단 전재 및 재배포 금지 \n",
      "\n",
      "역시,친일파들.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "동영상 뉴스3 1절 공휴일을 맞아 바깥나들이를 계획하신 분들 많으셨을 텐데요.전국 거의 모든 지역에서 미세먼지가 말썽입니다.정치훈 기자가 보도합니다.태극기 너머 전광판에 빨간불이 켜졌습니다.휴일을 맞아 나들이에 나섰지만, 답답한 공기 때문에 마스크를 벗기가 두렵습니다. 인터뷰 윤별이 경기 화성시 모처럼 밖에 산책하러 나왔는데 미세먼지도 심하고 황사 때문에 옥에 티가 있는 것 같아요. 무등산은 아예 사라졌습니다.지난 미세먼지 주의보 때와 비교해봐도 더 심하다는 것을 알 수 있습니다. 스탠딩 정치훈 기자 광주 전남에는 관측 이래 처음으로 초미세먼지 경보가 발효되는 등 전국 대부분 지역에 미세먼지 특보가 내려졌습니다. 초미세먼지 농도가 세제곱미터당 76마이크로그램을 넘어서면 매우 나쁨 수준이 되는데, 경보 지역에서는 2배를 넘어섰습니다.바람조차 없다 보니 유입된 미세먼지가 빠져나가지 못해 농도가 짙어진 겁니다.바깥나들이를 포기한 시민들은 실내로 모여들었습니다. 인터뷰 김보화 전남 진도군 미세먼지가 너무 심해서 야외활동 하기는 힘들 것 같아서 실내 공간을 찾다 보니까 오게 됐습니다. 오후 들어 경보가 주의보로 떨어졌지만, 여전히 고농도 초미세먼지가 관측됐습니다.또 밤사이 중국발 스모그가 한 차례 더 들어오면서 내일도 미세먼지가 매우 나쁨 수준을 나타낼 것으로 보입니다.MBN뉴스 정치훈입니다. 영상취재 최양규 기자 정영진 VJ영상편집 김경준 \n",
      "\n",
      "우리나라도 대통령이 있으면 좋겠다\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "여 야 정치권, 그나마 나은 선택 언론 트럼프 대북 접근법의 취약성 노출 미국 정치권과 언론은 불발 로 끝난 2차 북 미 정상회담 결과에 대해 나쁜 합의를 하는 것보다 걸어나오는 게 낫다 며 도널드 트럼프 대통령이 현명한 선택을 했다는 반응을 보였다. 김정은 북한 국무위원장과 과감한 정상 외교를 통해 북핵 문제를 단숨에 해결하려 했던 트럼프식 외교의 취약성을 지적하는 목소리도 이어졌다.트럼프 대통령과 갈등을 빚어온 낸시 펠로시 하원의장은 28일 기자회견에서 우리가 원한 것은 비핵화였다. 그들은 첫 만남에서 이에 동의하지 않았고, 두번째 만남에서도 동의하지 않았다. 그들은 비핵화 없이 제재 해제를 원했다. 나는 트럼프 대통령이 걸어나왔다는 사실을 기쁘게 생각한다 고 말했다. 이어 트럼프 대통령이 김정은과 두번 만난 끝에 그가 수준에 도달해 있지 않았다는 사실을 인식했다 며 트럼프 대통령이 그가 제안한 작은 것의 대가로 아무것도 주지 않은 것은 좋은 선택이었다 고 평가했다.트럼프 대통령과 가까운 공화당의 중진 린지 그레이엄 상원의원도 트위터에 나쁜 합의에 서명하는 것보다는 걸어나오는 게 낫다. 유일한 좋은 합의란 완전한 비핵화를 대가로 북한에 안전 보장과 경제적 지원을 제공하는 것 이라고 말했다. 그러나 그는 대화를 계속한다는 계획이 있어 다행 이라며 북한 핵 위협에 대해 평화적 결론에 도달하려는 트럼프 대통령의 노력을 평가한다 고 말했다. 척 슈머 민주당 상원 원내대표는 트럼프 대통령이 옳은 일을 했다. 나는 북한과의 갈등을 끝낼 협상을 원한다. 하지만 언제나 나쁜 합의의 가능성을 우려해왔다 고 말했다.일부 언론은 트럼프 대통령에게 대북 접근법을 수정하라고 요구했다. 는 사설에서 이번 합의 무산을 통해 트럼프 대통령이 지난해 6월의 첫 회담 이후 무언가를 배울 수 있었을 것 이라며 그는 북한이 더 이상 핵 위협이 아니다 라고 말했지만, 여전히 그렇다는 사실을 알게 됐다 고 지적했다. 도 정상회담의 실패는 점증하는 북핵 위협에 대한 트럼프 대통령의 전략이 근본적으로 취약한 것임을 노출했다. 그는 개인적 외교가 정권이 수십년 동안 저항해온 핵 폐기로 가는 중요한 발걸음을 이끌어낼 것으로 봤지만, 결과는 그가 걸어나올 수밖에 없었던 받아들일 수 없는 제안 이었다고 밝혔다. 또 정상회담을 통해 북핵 문제를 풀 수 있을 것이라는 트럼프 대통령의 오산이 북 미 관계를 혼란에 빠뜨렸다. 그의 실패는 미국의 접근법에 수정이 필요함을 보여준다 고 했다.길윤형 기자  \n",
      "\n",
      "북한의 비핵화 거짓말, 미사일 기지 추가 건설, 핵 저장 시설 추가 건설. 자. 비핵화는 조금도 없었고 평화는 조금도 더 다가오지 않았다. 이에 남북 철도 및 gp 파괴 등 국세 낭비한 문재인은 대단한 대가를 치뤄야만 할 것이다. 국민을 속인 북한의 공범이다\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "파리 연쇄테러 가담 혐의 프랑스 출신 IS 조직원 파비앙 클랭 하채림 특파원 2015년 파리 동시다발공격에 가담한 프랑스 출신 테러분자가 시리아에서 폭사했다는 언론보도가 사실이라고 국제동맹군이 확인했다.미군 주도 국제동맹군은 프랑스 출신 극단주의자 파비앙 클랭이 최근 시리아 동부 수니파 테러조직 이슬람국가 진영에서 공습으로 사망했다고 지난달 28일 밝혔다.IS 격퇴 국제동맹군은 트위터 계정에 동맹군 공습으로 다에시 미디어 당국자로 활동한 아부 아나스 알파란시, 즉 파비앙 클랭이 바구즈에서 사망했다 고 썼다.국제동맹군은 클랭의 사망 시점이 정확히 언제인지는 공개하지 않았다.앞서 지난달 21일 프랑스 언론은 클랭이 국제동맹군의 공습으로 제거됐으며, 그 형제 장미셸도 크게 다쳤다고 보도했다.프랑스 정부는 보도에 확인도 부인도 하지 않았다.2015년 11월 프랑스 파리에서 동시다발 공격으로 129명이 숨진 테러 후 그 배후를 자처하는 IS의 선전 영상에 등장한 목소리의 장본인이 클랭이다.프랑스 정보당국은 클랭 형제가 단순히 IS의 선전물 제작에 참여한 것뿐만 아니라 파리 연쇄테러의 기획에도 적극적으로 가담한 것으로 보고 소재를 추적했다 \n",
      "\n",
      "인권팔이 새끼들 집에 난민 하나씩 놔주자 \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "김정은 북한 국무위원장이 전날 도널드 트럼프 미 대통령과 북미정상회담 합의문 도출에 실패한 채 끝난 이후 베트남에서의 첫 공개 행보를 시작했다.김정은 북한 국무위원장과 응우옌 푸 쫑 베트남 공산당 서기장 겸 국가주석이 1일 오후 베트남 주석궁에서 의장사열하고 있다. 김 위원장은 1일 오후 베트남 주석궁을 방문하는 것으로 공식친선방문 일정을 시작했다. 이날 오후 3시 20분경 숙소 멜리아 호텔을 떠난 김 위원장은 오후 3시 30분 하노이 바딘 광장에 위치한 베트남 주석궁에 도착했다. 응우옌 푸 쫑 베트남 공산당 서기장 겸 국가주석이 직접 김 위원장을 맞이했고 주석궁 앞에서 의장사열 등 환영행사가 이어졌다.김 위원장이 숙소인 멜리아 호텔을 나서고 있다. 김 위원장의 차량이 호위를 받으며 주석궁으로 향하고 있다. 김 위원장이 1일 베트남 하노이 주석궁으로 들어가고 있다. 환영행사를 마친 김 위원장은 응우옌 푸 쫑 베트남 공산당 서기장 겸 국가주석과 양자 정상회담을 갖고 베트남 권력서열 2, 3위인 응우옌 쑤언 픅 총리와 응우옌 티 낌 응언 국회의장을 차례로 면담할 예정이다. 이어 저녁에는 국제컨벤션센터에서 열리는 환영 만찬에 참석한다.김정은 북한 국무위원장이 1일 오후 베트남 주석궁에서 환영행사에 참석하고 있다. 김 위원장은 2일 오전 호찌민 주석 묘를 방문해 헌화하고 정오께 동당 역에서 전용 열차로 귀국길에 오를 것으로 예상된다.장진영 기자 중앙일보, 무단 전재 및 재배포 금지 \n",
      "\n",
      "영변 말고 두곳이 더 있다라, 문모씨는 애초부터 북한비핵화는 관심이 없었던것임. 이니작곡 으니작사 짝짜궁에 트럼프가 같이 놀아날 줄 알았나? 아주아주 고소하구만 \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "하경민 기자 3 1절 100주년인 1일 오후 부산 동구 일본총영사관 인근 정발 장군 동상 앞에 강제징용 노동자상이 놓여진 가운데 3 1운동 100주년 부산시민대회 가 열리고 있다. 2019.03.01. 하경민 기자 부산 강제징용 노동자상이 일본총영사관 인근 정발 장군 동상 앞에 임시 설치됐다.적폐청산 사회대개혁 부산운동본부 강제징용노동자상 건립특별위원회는 3 1절 100주년인 1일 오후 부산 동구 초량동 정발 장군 동상 앞에서 400여 명이 참석한 가운데 3 1운동 100주년 부산시민대회 를 개최했다.주최 측은 본 행사에 앞서 파손 부분의 보수를 마친 노동자상을 약 8개월 만에 정발 장군 동상 앞 횡단보도 뒤에 임시 설치했다.노동자상 건립위는 부산시민의 힘으로 만들어진 노동자상을 일본영사관 앞 평화의 소녀상 옆에 설치하겠다는 방침은 변함이 없다 면서 정부, 부산시, 동구 등에 노동자상 설치를 위한 협상을 요청했다 고 밝혔다.이날 부산시민대회 참가자들은 50분 만에 행사를 마친 이후 인도를 이용해 일본총영사관 앞으로 거리행진을 시도했지만, 경찰은 이를 허용하지 않고 대치했다.참가자들은 대치 중인 경찰과 수 차례에 걸쳐 몸싸움을 벌이며 행진을 시도했지만 경찰에 가로막혀 한 발로 나아가지 못했고, 대치 상황은 1시간 만에 참가자들이 자진 해산하면서 종료됐다. 하경민 기자 1일 부산 동구 초량동 일본총영사관 인근 정발 장군 동상 앞에서 열린 3 1운동 100주년 부산시민대회 에 참가한 시민단체 회원과 시민 등이 일본영사관 방향으로 행진하다 경찰과 대치하고 있다. 2019.03.01. 한편 노동자상 건립위는 지난해 4월 30일 밤 일본영사관 앞 평화의 소녀상 옆에 노동자상을 기습 설치하려다가 경찰의 저지에 의해 막혀 노동자상은 인도 한복판에 놓여졌고, 지난 5월 31일 동구청이 노동자상 강제철거 행정대집행을 실시한 이후 남구 일제강제동원역사관에 임시보관 조치했다.구는 건립위가 노동자상을 일본영사관 앞에 다시 설치할 것을 우려해 노동자상을 반환하지 않았고, 이에 건립위는 최근 노동자상 반환을 위한 가처분신청을 제기하는 등 갈등을 겪어 왔다.이후 행정안전부는 노동자상 인도에 대한 내용이 담긴 공문을 구청으로 보냈고, 동구는 행정대집행 비용 110만원을 건립위가 납부하자 지난해 7월 4일 노동자상을 건립위에 반환했다. 건립위는 노동자상 파손 부위 보수를 보냈다가 약 8개월 만인 이날 공개했다 \n",
      "\n",
      "역사를 잊은 민족은 미래가 없다. 과거에 얽매인다기 보다는 이런 비극이 다시는 일어나지 않게 해야함. 뼛속까지 새겨도 모자름.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with open('../Data/Preprocessed/title.txt', 'r') as title_file:\n",
    "#     title_whole = title_file.read()\n",
    "# with open('../Data/Preprocessed/subject.txt', 'r') as subject_file:\n",
    "#     subject_whole = subject_file.read()\n",
    "with open('../Data/Preprocessed/reply.txt', 'r') as reply_file:\n",
    "    reply_whole = reply_file.read()\n",
    "with open('../Data/Preprocessed/body.txt', 'r') as body_file:\n",
    "    body_whole = body_file.read()\n",
    "    \n",
    "# 전체 텍스트의 마지막 빈 문장 제거 및 텍스트 리스트로 값 저장\n",
    "# titles = title_whole.rstrip().split('\\n')\n",
    "# subjects = subject_whole.rstrip().split('\\n')\n",
    "replies = reply_whole.rstrip().split('\\n')\n",
    "bodies = body_whole.rstrip().split('\\n')\n",
    "\n",
    "# 파일에서 불러온 텍스트의 개수 확인\n",
    "# assert(len(titles) == len(subjects))\n",
    "assert(len(bodies) == len(replies))\n",
    "print('before deleting :', len(bodies))\n",
    "\n",
    "index_for_delete = []\n",
    "for i in range(len(bodies)):\n",
    "    if bodies[i] == '':\n",
    "        index_for_delete.append(i)\n",
    "\n",
    "for index in index_for_delete:\n",
    "    del bodies[index]\n",
    "    del replies[index]\n",
    "\n",
    "assert(len(bodies) == len(replies))\n",
    "print('after deleting :', len(bodies))\n",
    "    \n",
    "# 단순 List<String>의 형태였던 변수를 List<List<Dictionary>> 형태로 변환한다\n",
    "for i in range(len(replies)):\n",
    "    replies[i] = eval(replies[i])\n",
    "best_replies = []\n",
    "# 5개의 샘플 확인\n",
    "for i in range(10):\n",
    "    print(bodies[i],'\\n')\n",
    "    index_best_reply = np.argmax([reply['good'] - reply['bad'] for reply in replies[i]])\n",
    "    best_replies.append(replies[i][index_best_reply]['text'])\n",
    "    print(replies[i][index_best_reply]['text'])\n",
    "    print('-'*120)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "def preprocess_body(line):\n",
    "    line = sub('[0-9]{4}\\.[0-9]{2}\\.[0-9]{2}', '', line)\n",
    "    line = sub('...\\s기자', '', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "텍스트 전처리 함수 정의\n",
    "\n",
    "텍스트를 argument로 받아서 전처리된 새로운 텍스트를 결과로 반환한다.\n",
    "\n",
    "'''\n",
    "\n",
    "from re import sub\n",
    "\n",
    "def preprocess_texts(line):\n",
    "    line = sub('동영상 뉴스', '', line)\n",
    "#     line = sub('....기자.{1,3}\\s', '', line)\n",
    "    # 2019.08.23 과 같은 날짜 문자열 삭제\n",
    "    line = sub('[0-9]{4}\\.[0-9]{2}\\.[0-9]{2}', '', line)\n",
    "    # 괄호와 그 안의 단어 삭제\n",
    "    line = sub('(\\(.*?\\))|(\\[.*?\\])|(\\<.*?\\>)|(【.*?】)', '', line)\n",
    "    # 문장 동그라미 숫자 삭제\n",
    "    line = sub('[①-⑳]', '', line)\n",
    "    # 기자 이메일 주소 삭제\n",
    "    line = sub('[a-zA-Z0-9]{,15}@[a-zA-Z]{,15}[(com)(kr)]', '', line)\n",
    "    # 빈칸과 . 제외한 모든 특수문자 제거\n",
    "    line = sub('(?!\\s)(\\W)(?!\\.)', ' ', line)\n",
    "    # 왼쪽 오른쪽 공백문자 제거\n",
    "    line = line.rstrip().lstrip()\n",
    "    # 여러번 반복되는 공백문자 한칸으로 축소\n",
    "    line = sub('\\s+', ' ', line)\n",
    "    # 마무리 문장 제거(eg. @@@기자 ~ @@금지)\n",
    "#     line = sub('....기자.*금지', '', line)\n",
    "#     line = sub('{3,6}뉴스\\s', '', line)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(titles)):\n",
    "#     titles[i] = preprocess_texts(titles[i])\n",
    "#     bodies[i] = preprocess_texts(bodies[i])\n",
    "\n",
    "# for i in range(len(replies)):\n",
    "#     for j in range(len(replies[i])):\n",
    "#         replies[i][j]['text']= preprocess_texts(replies[i][j]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전처리된 텍스트 확인\n",
    "# # 5개의 샘플\n",
    "# for i in range(10):\n",
    "#     print('subject\\t:', subjects[i])\n",
    "#     print('title\\t:', titles[i])\n",
    "#     print('body\\t:')\n",
    "#     print(bodies[i])\n",
    "#     for j in range(5):\n",
    "#         print(replies[i][j]['text'])\n",
    "#         print('good\\t:', replies[i][j]['good'])\n",
    "#         print('bad\\t:', replies[i][j]['bad'])\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 포스 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import codecs\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Okt()\n",
    "corpus_body = codecs.open('corpus_body.txt', 'w', encoding='utf-8')\n",
    "corpus_reply = codecs.open('corpus_reply.txt', 'w', encoding='utf-8')\n",
    "\n",
    "corpus_body_v = codecs.open('corpus_body_v.txt', 'w', encoding='utf-8')\n",
    "corpus_reply_v = codecs.open('corpus_reply_v.txt', 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat(content):\n",
    "    return [\"{}/{}\".format(word, tag) for word, tag in tagger.pos(content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies_v = bodies[3500:-1]\n",
    "replies_v = replies[3500:-1]\n",
    "\n",
    "bodies = bodies[:3500]\n",
    "replies = replies[:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "for body in bodies:\n",
    "    corpus_body.write(' '.join(flat(body)) + '\\n')\n",
    "for reply in replies:\n",
    "    for r in reply:\n",
    "        corpus_reply.write(' '.join(flat(r['text'])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "for body in bodies_v:\n",
    "    corpus_body_v.write(' '.join(flat(body)) + '\\n')\n",
    "for reply in replies_v:\n",
    "    for r in reply:\n",
    "        corpus_reply_v.write(' '.join(flat(r['text'])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceReader:\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in codecs.open(self.filepath, encoding='utf-8'):\n",
    "            yield line.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9042613, 11298555)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주의!! 시간 꽤 걸림\n",
    "sentences_vocab_b = SentenceReader('corpus_body.txt')\n",
    "sentences_train_b = SentenceReader('corpus_body.txt')\n",
    "\n",
    "model_b = gensim.models.Word2Vec()\n",
    "model_b.build_vocab(sentences_vocab_b)\n",
    "model_b.train(sentences_train_b, total_examples=model_b.corpus_count, epochs=model_b.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100221, 2750030)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_vocab_b_v = SentenceReader('corpus_body_v.txt')\n",
    "sentences_train_b_v = SentenceReader('corpus_body_v.txt')\n",
    "\n",
    "model_b_v = gensim.models.Word2Vec()\n",
    "model_b_v.build_vocab(sentences_vocab_b_v)\n",
    "model_b_v.train(sentences_train_b_v, total_examples=model_b_v.corpus_count, epochs=model_b_v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3595826, 4576540)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 주의!! 시간 꽤 걸림\n",
    "sentences_vocab_r = SentenceReader('corpus_reply.txt')\n",
    "sentences_train_r = SentenceReader('corpus_reply.txt')\n",
    "\n",
    "model_r = gensim.models.Word2Vec()\n",
    "model_r.build_vocab(sentences_vocab_r)\n",
    "model_r.train(sentences_train_r, total_examples=model_r.corpus_count, epochs=model_r.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(781988, 1109355)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_vocab_r_v = SentenceReader('corpus_reply_v.txt')\n",
    "sentences_train_r_v = SentenceReader('corpus_reply_v.txt')\n",
    "\n",
    "model_r_v = gensim.models.Word2Vec()\n",
    "model_r_v.build_vocab(sentences_vocab_r_v)\n",
    "model_r_v.train(sentences_train_r_v, total_examples=model_r_v.corpus_count, epochs=model_r_v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.save('model_b')\n",
    "model_r.save('model_r')\n",
    "\n",
    "model_b_v.save('model_b_v')\n",
    "model_r_v.save('model_r_v')\n",
    "\n",
    "model_b = gensim.models.Word2Vec.load('model_b')\n",
    "model_r = gensim.models.Word2Vec.load('model_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('환경부/Noun', 0.6020535230636597),\n",
       " ('교육부/Noun', 0.5545091032981873),\n",
       " ('발표/Noun', 0.541748046875),\n",
       " ('국제사회/Noun', 0.5370264053344727),\n",
       " ('대응/Noun', 0.5338596105575562),\n",
       " ('의회/Noun', 0.5338441133499146),\n",
       " ('국토부/Noun', 0.5311428904533386),\n",
       " ('협의/Noun', 0.5225166082382202),\n",
       " ('국가/Noun', 0.513191819190979),\n",
       " ('복지부/Noun', 0.5117552876472473)]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Vocab 확인\n",
    "model_b.wv.most_similar(['정부/Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('총리/Noun', 0.8634436130523682),\n",
       " ('청문회/Noun', 0.861016035079956),\n",
       " ('교수/Noun', 0.8433427214622498),\n",
       " ('관계자/Noun', 0.827811062335968),\n",
       " ('후보자/Noun', 0.8253326416015625),\n",
       " ('사퇴/Noun', 0.8119133710861206),\n",
       " ('질의/Noun', 0.807749330997467),\n",
       " ('지명/Noun', 0.8029180765151978),\n",
       " ('우리/Noun', 0.8024487495422363),\n",
       " ('재무부/Noun', 0.8023450970649719)]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b_v.wv.most_similar(['정부/Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정권/Noun', 0.6727776527404785),\n",
       " ('평화/Noun', 0.6505265235900879),\n",
       " ('이제/Noun', 0.6183399558067322),\n",
       " ('트럼프/Noun', 0.6112842559814453),\n",
       " ('자체/Noun', 0.6095144152641296),\n",
       " ('우리/Noun', 0.5965460538864136),\n",
       " ('혈맹입니/Noun', 0.5940648317337036),\n",
       " ('죄/Noun', 0.593606173992157),\n",
       " ('헌신/Noun', 0.589924156665802),\n",
       " ('엔지니어/Noun', 0.5887530446052551)]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Vocab 확인\n",
    "model_r.wv.most_similar(['정부/Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정권/Noun', 0.9965959787368774),\n",
       " ('통/Noun', 0.995354413986206),\n",
       " ('탄핵/Noun', 0.9952712059020996),\n",
       " ('국방/Noun', 0.995102047920227),\n",
       " ('먼지/Noun', 0.9950155019760132),\n",
       " ('뽑은/Verb', 0.9946746826171875),\n",
       " ('전/Modifier', 0.9944210052490234),\n",
       " ('매국노/Noun', 0.994355320930481),\n",
       " ('앙/Adverb', 0.9943262338638306),\n",
       " ('더불어/Verb', 0.9943023920059204)]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_r_v.wv.most_similar(['정부/Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[5, 6],\n",
       "        [7, 8]]])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[[1,2],[3,4]],[[5,6],[7,8]]]\n",
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 21529 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(None, dtype=object)"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('corpus_body.txt') as body_corpus:\n",
    "    body_whole_tagged = body_corpus.read()\n",
    "    \n",
    "bodies_tagged = body_whole_tagged.split('\\n')\n",
    "\n",
    "num_body = len(bodies_tagged)\n",
    "num_vocab = len(model_b.wv.vocab)\n",
    "num_dim = 100\n",
    "print(num_body, num_vocab, num_dim)\n",
    "\n",
    "\n",
    "_x_train = []\n",
    "for body in bodies_tagged:\n",
    "    words = body.split(' ')\n",
    "    for word in words:\n",
    "        x_train_sub = []\n",
    "        try:\n",
    "            word_index = model_b.wv.vocab[word].index\n",
    "            x_train_sub.append(model_b.wv.vectors[word_index])\n",
    "        except:\n",
    "            x_train_sub.append(np.zeros(100))\n",
    "    _x_train.append(x_train_sub)\n",
    "    x_trian = np.array(_x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open('corpus_body_v.txt') as body_corpus_v:\n",
    "#     body_whole_tagged_v = body_corpus_v.read()\n",
    "    \n",
    "# bodies_tagged_v = body_whole_tagged_v.split('\\n')\n",
    "\n",
    "# x_val = []\n",
    "# for body in bodies_tagged_v:\n",
    "#     x_val_sub = []\n",
    "#     words = body.split(' ')\n",
    "#     for word in words:\n",
    "#         try:\n",
    "#             word_index = model_b_v.wv.vocab[word].index            \n",
    "#             x_val_sub.append(model_b_v.wv.vectors[word_index])\n",
    "#         except:\n",
    "#             x_val_sub.append(np.zeros(100))\n",
    "#     x_val.append(x_val_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_reply.txt') as reply_corpus:\n",
    "    reply_whole_tagged = reply_corpus.read()\n",
    "    \n",
    "replies_tagged = reply_whole_tagged.split('\\n')\n",
    "\n",
    "y_train = []\n",
    "for reply in replies_tagged:\n",
    "    words = reply.split(' ')\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_index = model_r.wv.vocab[word].index            \n",
    "            y_train.append(model_r.wv.vectors[word_index])\n",
    "        except:\n",
    "            y_train.append(np.zeros(100))\n",
    "            \n",
    "with open('corpus_reply_v.txt') as reply_corpus_v:\n",
    "    reply_whole_tagged_v = reply_corpus_v.read()\n",
    "    \n",
    "replies_tagged_v = reply_whole_tagged_v.split('\\n')\n",
    "\n",
    "y_val = []\n",
    "for reply in replies_tagged_v:\n",
    "    words = reply.split(' ')\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_index = model_r_v.wv.vocab[word].index            \n",
    "            y_val.append(model_r_v.wv.vectors[word_index])\n",
    "        except:\n",
    "            y_val.append(np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2259712, 250)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.models import AttentionSeq2Seq\n",
    "\n",
    "num_samples = len(bodies)\n",
    "input_length = 50\n",
    "input_dim = 100\n",
    "\n",
    "models = AttentionSeq2Seq(input_dim=100, input_length=50, hidden_dim=100, output_length=50, output_dim=100, depth=6)\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x,y, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import sequence\n",
    "# max_sentence_length = 250\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=max_sentence_length)\n",
    "# x_val = sequence.pad_sequences(x_val, maxlen=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n",
      "5.052934646606445\n"
     ]
    }
   ],
   "source": [
    "# input_shape = x_train.shape[1:]\n",
    "# output_shape = np.max(y_train) + 1\n",
    "# print(input_shape)\n",
    "# print(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Flatten, Dropout, Input\n",
    "# from keras.layers import LSTM, SimpleRNN, GRU\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer gru_1: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-317-381cfcecb448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer gru_1: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# input_layer = Input(shape=input_shape)\n",
    "# layer = input_layer\n",
    "\n",
    "# layer = BatchNormalization()(layer)\n",
    "# layer = GRU(64, dropout=0.45, recurrent_dropout=0.45)(layer)\n",
    "# layer = Dropout(0.45)(layer)\n",
    "# layer = BatchNormalization()(layer)\n",
    "\n",
    "# layer = Dense(output_shape, activation='softmax')(layer)\n",
    "\n",
    "# output_layer = layer\n",
    "# model = Model(inputs=[input_layer], outputs=[output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
